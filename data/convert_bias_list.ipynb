{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24888e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ ghi 34358 d√≤ng JSON v√†o 'all_texts_train.jsonl'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "root_dir = \"medical-united-syn-med-75/train\"\n",
    "output_file = \"all_texts_train.jsonl\"\n",
    "\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    index = 0\n",
    "\n",
    "    \n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith(\".json\"):\n",
    "                filepath = os.path.join(dirpath, filename)\n",
    "                try:\n",
    "                    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                        data = json.load(f)\n",
    "                        if \"text\" in data:\n",
    "                            json_line = {\n",
    "                                \"id\": f\"{index:06d}\",\n",
    "                                \"file\": filename,\n",
    "                                \"text\": data[\"text\"].strip()\n",
    "                            }\n",
    "                            out_file.write(json.dumps(json_line, ensure_ascii=False) + \"\\n\")\n",
    "                            index += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"L·ªói khi ƒë·ªçc file {filepath}: {e}\")\n",
    "\n",
    "print(f\"ƒê√£ ghi {index} d√≤ng JSON v√†o '{output_file}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c547b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ ghi 1/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 100/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 200/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 300/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 400/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 500/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 600/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 700/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 800/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 900/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1000/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1100/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1200/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1300/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1400/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1500/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1600/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1700/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1800/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 1900/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2000/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2100/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2200/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2300/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2400/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2500/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2600/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2700/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2800/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 2900/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3000/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3100/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3200/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3300/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3400/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3500/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3600/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3700/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3800/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 3900/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4000/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4100/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4200/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4300/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4400/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4500/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4600/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4700/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4800/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 4900/5000 d√≤ng\n",
      "‚úÖ ƒê√£ ghi 5000/5000 d√≤ng\n",
      "\n",
      "üéâ Ho√†n t·∫•t! ƒê√£ ghi t·ªïng c·ªông 5000 d√≤ng v√†o train_dev_5000_suffer.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "\n",
    "train_path = \"all_texts_train.jsonl\"\n",
    "dev_path = \"all_texts_dev.jsonl\"\n",
    "output_path = \"train_dev_5000_suffer.jsonl\"\n",
    "\n",
    "\n",
    "train_sample_size = 4250\n",
    "dev_sample_size = 750\n",
    "\n",
    "\n",
    "with open(train_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    train_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "\n",
    "with open(dev_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    dev_data = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "\n",
    "train_sample = random.sample(train_data, train_sample_size)\n",
    "dev_sample = random.sample(dev_data, dev_sample_size)\n",
    "\n",
    "\n",
    "combined_data = train_sample + dev_sample\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    for idx, item in enumerate(combined_data, 1):\n",
    "        out_f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "        if idx % 100 == 0 or idx == 1 or idx == len(combined_data):\n",
    "            print(f\"‚úÖ ƒê√£ ghi {idx}/{len(combined_data)} d√≤ng\")\n",
    "\n",
    "print(f\"\\nHo√†n t·∫•t! ƒê√£ ghi t·ªïng c·ªông {len(combined_data)} d√≤ng v√†o {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e96d857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ƒê√£ ghi 5000 d√≤ng vƒÉn b·∫£n v√†o train_dev_5000_suffer.txt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_path = \"train_dev_5000_suffer.jsonl\"\n",
    "output_txt = \"train_dev_5000_suffer.txt\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as in_f, open(output_txt, \"w\", encoding=\"utf-8\") as out_f:\n",
    "    count = 0\n",
    "    for line in in_f:\n",
    "        try:\n",
    "            obj = json.loads(line.strip())\n",
    "            text = obj.get(\"text\", \"\").strip()\n",
    "            if text:\n",
    "                out_f.write(text + \"\\n\")\n",
    "                count += 1\n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"L·ªói d√≤ng kh√¥ng h·ª£p l·ªá: {line[:50]}...\")\n",
    "\n",
    "print(f\"ƒê√£ ghi {count} d√≤ng vƒÉn b·∫£n v√†o {output_txt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9769b7c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44315"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"all_texts.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    entries = [json.loads(line.strip()) for line in f]\n",
    "\n",
    "total = len(entries)\n",
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6a8fef25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1842acb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Suclosz, a prominent drugchemical, is a promising medication under research for its potential therapeutic effects on diabetes, equipped with unique formulation elements aiding in precise administration.\"'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"GPT_API_TOKEN\"),\n",
    ")\n",
    "\n",
    "\n",
    "transcript = \"Suclosz is a new medication currently being studied for its potential benefits.\"\n",
    "\n",
    "\n",
    "prompt = f\"\"\"For medical transcript: \"{transcript}\"\n",
    "\n",
    "Create a concise description (1 sentence) that:\n",
    "1. Identifies terms from categories: drugchemical, diagnostics, meddevicetechnique\n",
    "2. Explains their medical purpose and context\n",
    "3. Includes distinctive features to help with speech recognition\n",
    "\n",
    "If no specialized terms exist, provide brief medical context.\"\"\"\n",
    "\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": prompt\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0dfc3ced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "204\n"
     ]
    }
   ],
   "source": [
    "print(len(response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a490c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference kh√¥ng c√≥ description:\n",
      " Cuclos is a new medication currently being studied for its potential benefits.\n",
      "\n",
      "Inference c√≥ description:\n",
      " is a new medication currently being studied for its potential benefits.\n",
      "reference:Suclosz is a new medication currently being studied for its potential benefits.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "model_id = \"openai/whisper-base\"\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(model_id)\n",
    "forced_decoder_ids = processor.get_decoder_prompt_ids(language=\"en\", task=\"transcribe\")\n",
    "\n",
    "audio_input, sample_rate = librosa.load(audio_path, sr=16000)\n",
    "input_features = processor(audio_input, sampling_rate=16000, return_tensors=\"pt\").input_features\n",
    "\n",
    "audio_path = \"medical-united-syn-med-75/test/drug-brand-en-us-female-e6c985fc-4c02-46ea-b8e7-b3ce1a7048b9.mp3\"\n",
    "\n",
    "\n",
    "description = \"healthcare transcription with medical terminology and medications\"\n",
    "\n",
    "\n",
    "def predict_with_whisper(input_features, description=None):\n",
    "    if description:\n",
    "        \n",
    "        \n",
    "        prompt = f\"<SOP> {description} <SOT>\"\n",
    "        \n",
    "        \n",
    "        prompt_ids = processor.tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "        \n",
    "        \n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            forced_decoder_ids=forced_decoder_ids,\n",
    "            decoder_input_ids=prompt_ids\n",
    "        )\n",
    "    else:\n",
    "        \n",
    "        predicted_ids = model.generate(\n",
    "            input_features,\n",
    "            forced_decoder_ids=forced_decoder_ids\n",
    "        )\n",
    "    \n",
    "    \n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "\n",
    "print(\"Inference kh√¥ng c√≥ description:\")\n",
    "result_without_desc = predict_with_whisper(input_features)\n",
    "print(result_without_desc)\n",
    "\n",
    "\n",
    "print(\"\\nInference c√≥ description:\")\n",
    "result_with_desc = predict_with_whisper(input_features, description)\n",
    "print(result_with_desc)\n",
    "\n",
    "print(f\"reference:\"\n",
    "      f\"{transcript}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedeada6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done ID: 011736\n",
      "Done ID: 011737\n",
      "Done ID: 011738\n",
      "Done ID: 011739\n",
      "Done ID: 011740\n",
      "Done ID: 011741\n",
      "Done ID: 011742\n",
      "Done ID: 011743\n",
      "Done ID: 011744\n",
      "Done ID: 011745\n",
      "Done ID: 011746\n",
      "Done ID: 011747\n",
      "Done ID: 011748\n",
      "Done ID: 011749\n",
      "Done ID: 011750\n",
      "Done ID: 011751\n",
      "Done ID: 011752\n",
      "Done ID: 011753\n",
      "Done ID: 011754\n",
      "Done ID: 011755\n",
      "Done ID: 011756\n",
      "Done ID: 011757\n",
      "Done ID: 011758\n",
      "Done ID: 011759\n",
      "Done ID: 011760\n",
      "Done ID: 011761\n",
      "Done ID: 011762\n",
      "Done ID: 011763\n",
      "Done ID: 011764\n",
      "Done ID: 011765\n",
      "Done ID: 011766\n",
      "Done ID: 011767\n",
      "Done ID: 011768\n",
      "Done ID: 011769\n",
      "Done ID: 011770\n",
      "Done ID: 011771\n",
      "Done ID: 011772\n",
      "Done ID: 011773\n",
      "Done ID: 011774\n",
      "Done ID: 011775\n",
      "Done ID: 011776\n",
      "Done ID: 011777\n",
      "Done ID: 011778\n",
      "Done ID: 011779\n",
      "Done ID: 011780\n",
      "Done ID: 011781\n",
      "Done ID: 011782\n",
      "Done ID: 011783\n",
      "Done ID: 011784\n",
      "Done ID: 011785\n",
      "Done ID: 011786\n",
      "Done ID: 011787\n",
      "Done ID: 011788\n",
      "Done ID: 011789\n",
      "Done ID: 011790\n",
      "Done ID: 011791\n",
      "Done ID: 011792\n",
      "Done ID: 011793\n",
      "Done ID: 011794\n",
      "Done ID: 011795\n",
      "Done ID: 011796\n",
      "Done ID: 011797\n",
      "Done ID: 011798\n",
      "Done ID: 011799\n",
      "Done ID: 011800\n",
      "Done ID: 011801\n",
      "Done ID: 011802\n",
      "Done ID: 011803\n",
      "Done ID: 011804\n",
      "Done ID: 011805\n",
      "Done ID: 011806\n",
      "Done ID: 011807\n",
      "Done ID: 011808\n",
      "Done ID: 011809\n",
      "Done ID: 011810\n",
      "Done ID: 011811\n",
      "Done ID: 011812\n",
      "Done ID: 011813\n",
      "Done ID: 011814\n",
      "Done ID: 011815\n",
      "Done ID: 011816\n",
      "Done ID: 011817\n",
      "Done ID: 011818\n",
      "Done ID: 011819\n",
      "Done ID: 011820\n",
      "Done ID: 011821\n",
      "Done ID: 011822\n",
      "Done ID: 011823\n",
      "Done ID: 011824\n",
      "Done ID: 011825\n",
      "Done ID: 011826\n",
      "Done ID: 011827\n",
      "Done ID: 011828\n",
      "Done ID: 011829\n",
      "Done ID: 011830\n",
      "Done ID: 011831\n",
      "Done ID: 011832\n",
      "Done ID: 011833\n",
      "Done ID: 011834\n",
      "Done ID: 011835\n",
      "Done ID: 011836\n",
      "Done ID: 011837\n",
      "Done ID: 011838\n",
      "Done ID: 011839\n",
      "Done ID: 011840\n",
      "Done ID: 011841\n",
      "Done ID: 011842\n",
      "Done ID: 011843\n",
      "Done ID: 011844\n",
      "Done ID: 011845\n",
      "Done ID: 011846\n",
      "Done ID: 011847\n",
      "Done ID: 011848\n",
      "Done ID: 011849\n",
      "Done ID: 011850\n",
      "Done ID: 011851\n",
      "Done ID: 011852\n",
      "Done ID: 011853\n",
      "Done ID: 011854\n",
      "Done ID: 011855\n",
      "Done ID: 011856\n",
      "Done ID: 011857\n",
      "Done ID: 011858\n",
      "Done ID: 011859\n",
      "Done ID: 011860\n",
      "Done ID: 011861\n",
      "Done ID: 011862\n",
      "Done ID: 011863\n",
      "Done ID: 011864\n",
      "Done ID: 011865\n",
      "Done ID: 011866\n",
      "Done ID: 011867\n",
      "Done ID: 011868\n",
      "Done ID: 011869\n",
      "Done ID: 011870\n",
      "Done ID: 011871\n",
      "Done ID: 011872\n",
      "Done ID: 011873\n",
      "Done ID: 011874\n",
      "Done ID: 011875\n",
      "Done ID: 011876\n",
      "Done ID: 011877\n",
      "Done ID: 011878\n",
      "Done ID: 011879\n",
      "Done ID: 011880\n",
      "Done ID: 011881\n",
      "Done ID: 011882\n",
      "Done ID: 011883\n",
      "Done ID: 011884\n",
      "Done ID: 011885\n",
      "Done ID: 011886\n",
      "Done ID: 011887\n",
      "Done ID: 011888\n",
      "Done ID: 011889\n",
      "Done ID: 011890\n",
      "Done ID: 011891\n",
      "Done ID: 011892\n",
      "Done ID: 011893\n",
      "Done ID: 011894\n",
      "Done ID: 011895\n",
      "Done ID: 011896\n",
      "Done ID: 011897\n",
      "Done ID: 011898\n",
      "Done ID: 011899\n",
      "Done ID: 011900\n",
      "Done ID: 011901\n",
      "Done ID: 011902\n",
      "Done ID: 011903\n",
      "Done ID: 011904\n",
      "Done ID: 011905\n",
      "Done ID: 011906\n",
      "Done ID: 011907\n",
      "Done ID: 011908\n",
      "Done ID: 011909\n",
      "Done ID: 011910\n",
      "Done ID: 011911\n",
      "Done ID: 011912\n",
      "Done ID: 011913\n",
      "Done ID: 011914\n",
      "Done ID: 011915\n",
      "Done ID: 011916\n",
      "Done ID: 011917\n",
      "Done ID: 011918\n",
      "Done ID: 011919\n",
      "Done ID: 011920\n",
      "Done ID: 011921\n",
      "Done ID: 011922\n",
      "Done ID: 011923\n",
      "Done ID: 011924\n",
      "Done ID: 011925\n",
      "Done ID: 011926\n",
      "Done ID: 011927\n",
      "Done ID: 011928\n",
      "Done ID: 011929\n",
      "Done ID: 011930\n",
      "Done ID: 011931\n",
      "Done ID: 011932\n",
      "Done ID: 011933\n",
      "Done ID: 011934\n",
      "Done ID: 011935\n",
      "Done ID: 011936\n",
      "Done ID: 011937\n",
      "Done ID: 011938\n",
      "Done ID: 011939\n",
      "Done ID: 011940\n",
      "Done ID: 011941\n",
      "Done ID: 011942\n",
      "Done ID: 011943\n",
      "Done ID: 011944\n",
      "Done ID: 011945\n",
      "Done ID: 011946\n",
      "Done ID: 011947\n",
      "Done ID: 011948\n",
      "Done ID: 011949\n",
      "Done ID: 011950\n",
      "Done ID: 011951\n",
      "Done ID: 011952\n",
      "Done ID: 011953\n",
      "Done ID: 011954\n",
      "Done ID: 011955\n",
      "Done ID: 011956\n",
      "Done ID: 011957\n",
      "Done ID: 011958\n",
      "Done ID: 011959\n",
      "Done ID: 011960\n",
      "Done ID: 011961\n",
      "Done ID: 011962\n",
      "Done ID: 011963\n",
      "Done ID: 011964\n",
      "Done ID: 011965\n",
      "Done ID: 011966\n",
      "Done ID: 011967\n",
      "Done ID: 011968\n",
      "Done ID: 011969\n",
      "Done ID: 011970\n",
      "Done ID: 011971\n",
      "Done ID: 011972\n",
      "Done ID: 011973\n",
      "Done ID: 011974\n",
      "Done ID: 011975\n",
      "Done ID: 011976\n",
      "Done ID: 011977\n",
      "Done ID: 011978\n",
      "Done ID: 011979\n",
      "Done ID: 011980\n",
      "Done ID: 011981\n",
      "Done ID: 011982\n",
      "Done ID: 011983\n",
      "Done ID: 011984\n",
      "Done ID: 011985\n",
      "Done ID: 011986\n",
      "Done ID: 011987\n",
      "Done ID: 011988\n",
      "Done ID: 011989\n",
      "Done ID: 011990\n",
      "Done ID: 011991\n",
      "Done ID: 011992\n",
      "Done ID: 011993\n",
      "Done ID: 011994\n",
      "Done ID: 011995\n",
      "Done ID: 011996\n",
      "Done ID: 011997\n",
      "Done ID: 011998\n",
      "Done ID: 011999\n",
      "Done ID: 012000\n",
      "Done ID: 012001\n",
      "Done ID: 012002\n",
      "Done ID: 012003\n",
      "Done ID: 012004\n",
      "Done ID: 012005\n",
      "Done ID: 012006\n",
      "Done ID: 012007\n",
      "Done ID: 012008\n",
      "Done ID: 012009\n",
      "Done ID: 012010\n",
      "Done ID: 012011\n",
      "Done ID: 012012\n",
      "Done ID: 012013\n",
      "Done ID: 012014\n",
      "Done ID: 012015\n",
      "Done ID: 012016\n",
      "Done ID: 012017\n",
      "Done ID: 012018\n",
      "Done ID: 012019\n",
      "Done ID: 012020\n",
      "Done ID: 012021\n",
      "Done ID: 012022\n",
      "Done ID: 012023\n",
      "Done ID: 012024\n",
      "Done ID: 012025\n",
      "Done ID: 012026\n",
      "Done ID: 012027\n",
      "Done ID: 012028\n",
      "Done ID: 012029\n",
      "Done ID: 012030\n",
      "Done ID: 012031\n",
      "Done ID: 012032\n",
      "Done ID: 012033\n",
      "Done ID: 012034\n",
      "Done ID: 012035\n",
      "Done ID: 012036\n",
      "Done ID: 012037\n",
      "Done ID: 012038\n",
      "Done ID: 012039\n",
      "Done ID: 012040\n",
      "Done ID: 012041\n",
      "Done ID: 012042\n",
      "Done ID: 012043\n",
      "Done ID: 012044\n",
      "Done ID: 012045\n",
      "Done ID: 012046\n",
      "Done ID: 012047\n",
      "Done ID: 012048\n",
      "Done ID: 012049\n",
      "Done ID: 012050\n",
      "Done ID: 012051\n",
      "Done ID: 012052\n",
      "Done ID: 012053\n",
      "Done ID: 012054\n",
      "Done ID: 012055\n",
      "Done ID: 012056\n",
      "Done ID: 012057\n",
      "Done ID: 012058\n",
      "Done ID: 012059\n",
      "Done ID: 012060\n",
      "Done ID: 012061\n",
      "Done ID: 012062\n",
      "Done ID: 012063\n",
      "Done ID: 012064\n",
      "Done ID: 012065\n",
      "Done ID: 012066\n",
      "Done ID: 012067\n",
      "Done ID: 012068\n",
      "Done ID: 012069\n",
      "Done ID: 012070\n",
      "Done ID: 012071\n",
      "Done ID: 012072\n",
      "Done ID: 012073\n",
      "Done ID: 012074\n",
      "Done ID: 012075\n",
      "Done ID: 012076\n",
      "Done ID: 012077\n",
      "Done ID: 012078\n",
      "Done ID: 012079\n",
      "Done ID: 012080\n",
      "Done ID: 012081\n",
      "Done ID: 012082\n",
      "Done ID: 012083\n",
      "Done ID: 012084\n",
      "Done ID: 012085\n",
      "Done ID: 012086\n",
      "Done ID: 012087\n",
      "Done ID: 012088\n",
      "Done ID: 012089\n",
      "Done ID: 012090\n",
      "Done ID: 012091\n",
      "Done ID: 012092\n",
      "Done ID: 012093\n",
      "Done ID: 012094\n",
      "Done ID: 012095\n",
      "Done ID: 012096\n",
      "Done ID: 012097\n",
      "Done ID: 012098\n",
      "Done ID: 012099\n",
      "Done ID: 012100\n",
      "Done ID: 012101\n",
      "Done ID: 012102\n",
      "Done ID: 012103\n",
      "Done ID: 012104\n",
      "Done ID: 012105\n",
      "Done ID: 012106\n",
      "Done ID: 012107\n",
      "Done ID: 012108\n",
      "Done ID: 012109\n",
      "Done ID: 012110\n",
      "Done ID: 012111\n",
      "Done ID: 012112\n",
      "Done ID: 012113\n",
      "Done ID: 012114\n",
      "Done ID: 012115\n",
      "Done ID: 012116\n",
      "Done ID: 012117\n",
      "Done ID: 012118\n",
      "Done ID: 012119\n",
      "Done ID: 012120\n",
      "Done ID: 012121\n",
      "Done ID: 012122\n",
      "Done ID: 012123\n",
      "Done ID: 012124\n",
      "Done ID: 012125\n",
      "Done ID: 012126\n",
      "Done ID: 012127\n",
      "Done ID: 012128\n",
      "Done ID: 012129\n",
      "Done ID: 012130\n",
      "Done ID: 012131\n",
      "Done ID: 012132\n",
      "Done ID: 012133\n",
      "Done ID: 012134\n",
      "Done ID: 012135\n",
      "Done ID: 012136\n",
      "Done ID: 012137\n",
      "Done ID: 012138\n",
      "Done ID: 012139\n",
      "Done ID: 012140\n",
      "Done ID: 012141\n",
      "Done ID: 012142\n",
      "Done ID: 012143\n",
      "Done ID: 012144\n",
      "Done ID: 012145\n",
      "Done ID: 012146\n",
      "Done ID: 012147\n",
      "Done ID: 012148\n",
      "Done ID: 012149\n",
      "Done ID: 012150\n",
      "Done ID: 012151\n",
      "Done ID: 012152\n",
      "Done ID: 012153\n",
      "Done ID: 012154\n",
      "Done ID: 012155\n",
      "Done ID: 012156\n",
      "Done ID: 012157\n",
      "Done ID: 012158\n",
      "Done ID: 012159\n",
      "Done ID: 012160\n",
      "Done ID: 012161\n",
      "Done ID: 012162\n",
      "Done ID: 012163\n",
      "Done ID: 012164\n",
      "Done ID: 012165\n",
      "Done ID: 012166\n",
      "Done ID: 012167\n",
      "Done ID: 012168\n",
      "Done ID: 012169\n",
      "Done ID: 012170\n",
      "Done ID: 012171\n",
      "Done ID: 012172\n",
      "Done ID: 012173\n",
      "Done ID: 012174\n",
      "Done ID: 012175\n",
      "Done ID: 012176\n",
      "Done ID: 012177\n",
      "Done ID: 012178\n",
      "Done ID: 012179\n",
      "Done ID: 012180\n",
      "Done ID: 012181\n",
      "Done ID: 012182\n",
      "Done ID: 012183\n",
      "Done ID: 012184\n",
      "Done ID: 012185\n",
      "Done ID: 012186\n",
      "Done ID: 012187\n",
      "Done ID: 012188\n",
      "Done ID: 012189\n",
      "Done ID: 012190\n",
      "Done ID: 012191\n",
      "Done ID: 012192\n",
      "Done ID: 012193\n",
      "Done ID: 012194\n",
      "Done ID: 012195\n",
      "Done ID: 012196\n",
      "Done ID: 012197\n",
      "Done ID: 012198\n",
      "Done ID: 012199\n",
      "Done ID: 012200\n",
      "Done ID: 012201\n",
      "Done ID: 012202\n",
      "Done ID: 012203\n",
      "Done ID: 012204\n",
      "Done ID: 012205\n",
      "Done ID: 012206\n",
      "Done ID: 012207\n",
      "Done ID: 012208\n",
      "Done ID: 012209\n",
      "Done ID: 012210\n",
      "Done ID: 012211\n",
      "Done ID: 012212\n",
      "Done ID: 012213\n",
      "Done ID: 012214\n",
      "Done ID: 012215\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "import json\n",
    "\n",
    "client = OpenAI(api_key=os.getenv(\"GPT_API_TOKEN\"))\n",
    "\n",
    "input_path = \"all_texts_train.jsonl\"\n",
    "output_path = \"all_texts_train_description.jsonl\"\n",
    "\n",
    "start_from_id = \"011736\"\n",
    "end_at_id = \"034357\"\n",
    "\n",
    "existing_ids = set()\n",
    "if os.path.exists(output_path):\n",
    "    with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                item = json.loads(line)\n",
    "                existing_ids.add(item[\"id\"])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "def build_prompt(text):\n",
    "    return f\"\"\"For medical transcript: \"{text}\"\n",
    "\n",
    "Create a concise description (1 sentence) that:\n",
    "1. Identifies terms from categories: drugchemical, diagnostics, meddevicetechnique\n",
    "2. Explains their medical purpose and context\n",
    "3. Includes distinctive features to help with speech recognition\n",
    "\n",
    "If no specialized terms exist, provide brief medical context.\"\"\"\n",
    "\n",
    "with open(input_path, \"r\", encoding=\"utf-8\") as infile, open(output_path, \"a\", encoding=\"utf-8\") as outfile:\n",
    "    for line in infile:\n",
    "        try:\n",
    "            entry = json.loads(line)\n",
    "            entry_id = entry[\"id\"]\n",
    "\n",
    "            if entry_id < start_from_id:\n",
    "                continue\n",
    "            if end_at_id and entry_id > end_at_id:\n",
    "                print(f\"Reached end_at_id: {end_at_id}. Stopping.\")\n",
    "                break\n",
    "\n",
    "            text = entry[\"text\"]\n",
    "            prompt = build_prompt(text)\n",
    "\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                temperature=0.5,\n",
    "            )\n",
    "\n",
    "            description = response.choices[0].message.content.strip()\n",
    "            entry[\"description\"] = description\n",
    "\n",
    "            outfile.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n",
    "            print(f\"Done ID: {entry_id}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error at ID {entry.get('id', 'unknown')}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "609db544",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
